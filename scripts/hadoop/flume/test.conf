a1.sources = r_log
a1.sinks = k_hdfs k_kafka
a1.channels = c_hdfs c_kafka

# source: log
a1.sources.r_log.type = exec
a1.sources.r_log.command = tail -f /data/app/hadoop/logs/hadoop-$user-namenode-iZ8vb170kkirwkajpjnhonZ.log

a1.sources.r_log.channels = c_hdfs c_kafka

# sink: hdfs+kafka
# Text for readable && DataStream for delete timestamp in file
# Because SequenceFile need a key
a1.sinks.k_hdfs.channel = c_hdfs
a1.sinks.k_hdfs.type = hdfs
a1.sinks.k_hdfs.hdfs.path = hdfs:///flume/log/%Y-%m-%d
a1.sinks.k_hdfs.hdfs.writeFormat = Text
a1.sinks.k_hdfs.hdfs.fileType = DataStream
a1.sinks.k_hdfs.hdfs.filePrefix = hdfs.namenode.log
a1.sinks.k_hdfs.hdfs.rollInterval = 60
a1.sinks.k_hdfs.hdfs.rollSize = 16384
a1.sinks.k_hdfs.hdfs.rollCount = 100 

a1.sinks.k_kafka.channel = c_kafka
a1.sinks.k_kafka.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k_kafka.kafka.bootstrap.servers = localhost:9092
a1.sinks.k_kafka.kafka.topic = flume_log

# channel
a1.channels.c_hdfs.type = memory
a1.channels.c_hdfs.capacity = 2000
a1.channels.c_hdfs.transactionCapacity = 200

a1.channels.c_kafka.type = memory
a1.channels.c_kafka.capacity = 2000
a1.channels.c_kafka.transactionCapacity = 200
